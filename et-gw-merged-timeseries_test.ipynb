{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d425741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyhdf.SD import SD, SDC\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "from pyproj import CRS\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7afe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT INPUTS BASED ON SITE OF INTEREST\n",
    "\n",
    "# Single site to process\n",
    "SITE_NAME = \"MexicoAguascaliente\"\n",
    "\n",
    "# Year range\n",
    "START_YEAR = 2000\n",
    "END_YEAR = 2022\n",
    "\n",
    "# Paths\n",
    "sites_csv = \"/capstone/aridgw/data/2025_09_21_annual_median_groundwater_levels_16sites_tile_id.csv\"\n",
    "modis_data_dir = \"/capstone/aridgw/data/modis_data/\"\n",
    "output_csv = \"/capstone/aridgw/data/et_timeseries_mexico_aguascalientes_test.csv\"\n",
    "merged_output_csv = \"/capstone/aridgw/data/et_gw_merged_mexico_aguascalientes_test.csv\"  # Output for merged ET and GW data\n",
    "# Buffer radius in meters\n",
    "BUFFER_RADIUS = 1000  # 2 km\n",
    "\n",
    "# MODIS constants\n",
    "TILE_SIZE = 1111950.0\n",
    "X_MIN = -20015109.354\n",
    "Y_MAX = 10007554.677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee6cca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real MODIS filename example \n",
    "# MOD16A3GF.A2000001.h00v08.061.2020264071747.hdf\n",
    "# MOD16A3GF MODIS data naming convention \n",
    "# 2000 Year \n",
    "# Day of year 001\n",
    "# h00v08 Tile h=0, v=8\n",
    "# 061 = Collection number\n",
    "# 2020264071747 = Production date\n",
    "# {} Ensures there is a match for the specified number of digits\n",
    "# ({}) Captures name as a group after ensuring there is a match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb162a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used in processing\n",
    "\n",
    "def parse_modis_filename(filename):\n",
    "    \"\"\"Extract year, tile, h, v from MODIS filename\"\"\"\n",
    "    # Naming convention first \n",
    "    pattern = r'MOD16A3GF\\.A(\\d{4})\\d{3}\\.h(\\d{2})v(\\d{2})\\.\\d{3}\\.\\d+\\.hdf' # Defines regex pattern for MODIS filenames containing time and tile info\n",
    "    match = re.search(pattern, filename) # Looks through modis_data_dir for files that match the pattern\n",
    "    \n",
    "    if match:\n",
    "        return { # Returns a dictionary with extracted time and tile info\n",
    "            'year': int(match.group(1)),\n",
    "            'tile': f\"h{match.group(2)}v{match.group(3)}\",\n",
    "            'h': int(match.group(2)),\n",
    "            'v': int(match.group(3))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# def create_circle_buffer(center_lat, center_lon, radius_meters, target_crs):\n",
    "  #   \"\"\"Create circular buffer around site point\n",
    "    #    - Input lat/lon in degrees\n",
    "     #   - radius in meters\n",
    "      #  - target_crs: pyproj CRS object for target projection (MODIS Sinusoidal)\n",
    "    # \"\"\"\n",
    "    # point = gpd.GeoSeries([Point(center_lon, center_lat)], crs=\"EPSG:4326\") # CRS is EPSG:4326 by default since lat/lon are used\n",
    "    # point_projected = point.to_crs(epsg=3857) # Project to Web Mercator for accurate buffering in meters\n",
    "    # circle = point_projected.buffer(radius_meters) # Create buffer in meters\n",
    "    #circle_target = circle.to_crs(target_crs) # Convert to target CRS (MODIS Sinusoidal)\n",
    "    # return circle_target\n",
    "\n",
    "def create_circle_buffer(center_lat, center_lon, radius_meters, target_crs):\n",
    "    \"\"\"Create circular buffer around site point\"\"\"\n",
    "    point = gpd.GeoSeries([Point(center_lon, center_lat)], crs=\"EPSG:4326\")\n",
    "    # Project directly to target CRS (MODIS) \n",
    "    point_projected = point.to_crs(target_crs)\n",
    "    circle = point_projected.buffer(radius_meters)\n",
    "    return circle\n",
    "\n",
    "def extract_et_from_buffer(hdf_file, circle_geom, h, v):\n",
    "    \"\"\"Extract mean ET from buffer\n",
    "    - Inputs:\n",
    "        - hdf_file: path to MODIS HDF file\n",
    "        - circle_geom: output geometry of create_circle_buffer function in MODIS CRS\n",
    "        - h, v: tile indices\n",
    "    - Returns:\n",
    "        - mean ET value within buffer (float, mm/yr), or np.nan if no data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open HDF\n",
    "        hdf = SD(hdf_file, SDC.READ)\n",
    "        et_dataset = hdf.select('ET_500m')\n",
    "        scale_factor = et_dataset.scale_factor\n",
    "        fill_value = et_dataset.attributes()['_FillValue'] # Fill values should have been identified, they will turn into NaN\n",
    "        \n",
    "        # Read and clean data\n",
    "        et_data_raw = et_dataset[:].astype(float) # Turn into float for NaN handling\n",
    "        et_data_raw[et_data_raw == fill_value] = np.nan # Set fill values to NaN\n",
    "        et_data_raw[et_data_raw >= 65500] = np.nan # Additional cleaning for unrealistic values\n",
    "        et_data_raw[et_data_raw < 0] = np.nan # Additional cleaning for negative values\n",
    "        remaining_suspicious = et_data_raw[(et_data_raw >= 60000) & (~np.isnan(et_data_raw))]\n",
    "        if len(remaining_suspicious) > 0:\n",
    "            print(f\"  WARNING: {len(remaining_suspicious)} pixels with values >= 60,000 remain after filtering!\")\n",
    "            print(f\"  Values: {np.unique(remaining_suspicious)}\")\n",
    "        else:\n",
    "            valid_count = np.sum(~np.isnan(et_data_raw))\n",
    "            if valid_count > 0:\n",
    "                print(f\"    ✓ No fill values detected. Max raw value: {np.nanmax(et_data_raw):.0f}\")\n",
    "        et_data = et_data_raw * scale_factor # Scale to (mm/yr) units using scaling factor\n",
    "        \n",
    "        # Setup geotransform\n",
    "        PIXELS = et_data.shape[0] # Number of pixels in one dimension (assuming square)\n",
    "        RES = TILE_SIZE / PIXELS # Calculate resolution, how many meters per pixel\n",
    "        x_ul = X_MIN + h * TILE_SIZE # Upper-left x coordinate\n",
    "        y_ul = Y_MAX - v * TILE_SIZE # Upper-left y coordinate\n",
    "        transform = from_origin(x_ul, y_ul, RES, RES) # Create transformation\n",
    "        # modis_crs = CRS.from_proj4(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\") # MODIS Sinusoidal CRS OLD\n",
    "        # modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs +type=crs\")\n",
    "        modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs\") # MODIS Sinusoidal CRS\n",
    "        \n",
    "        # Create profile and clip\n",
    "        profile = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": et_data.shape[0], # Pixels in y direction\n",
    "            \"width\": et_data.shape[1], # Pixels in x direction\n",
    "            \"count\": 1, # of bands\n",
    "            \"dtype\": \"float32\", # Data type\n",
    "            \"crs\": modis_crs, # CRS info\n",
    "            \"transform\": transform, # Transformation info\n",
    "            \"nodata\": np.nan\n",
    "        }\n",
    "        \n",
    "        with MemoryFile() as memfile: # Use in-memory file for rasterio operations for efficiency\n",
    "            with memfile.open(**profile) as dataset:\n",
    "                dataset.write(et_data.astype(\"float32\"), 1)\n",
    "                clipped, _ = mask(dataset, circle_geom.geometry, crop=True, all_touched=True) # Clip to buffer area, have all ET pixels touched by buffer included\n",
    "        \n",
    "        # Calculate mean\n",
    "        clipped_valid = clipped[0][~np.isnan(clipped[0])] # Valid (non-NaN) values only\n",
    "        # Conduct mean zonal statistics\n",
    "        mean_et = clipped_valid.mean() if len(clipped_valid) > 0 else np.nan # Mean ET or NaN if no valid data\n",
    "        \n",
    "        hdf.end()\n",
    "        return mean_et\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8bb86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Extracting ET timeseries for: MexicoAguascaliente\n",
      "Years: 2000-2022\n",
      "======================================================================\n",
      "\n",
      " STEP 1: Loading site information from CSV\n",
      "✓ Location: MexicoAguascaliente\n",
      "✓ Coordinates: (22.198160, -102.310000)\n",
      "✓ MODIS Tile: h08v06\n",
      "\n",
      " STEP 2: Creating 1000m buffer\n",
      "✓ Buffer created\n",
      "\n",
      " STEP 3: Finding MODIS files for tile h08v06\n",
      "✓ Found 7139 total MODIS files\n",
      "✓ Filtered to 23 files for h08v06 (2000-2022)\n",
      "\n",
      " STEP 4: Extracting ET values\n",
      "\n",
      "Year     Mean ET (mm/yr)      Status\n",
      "--------------------------------------------------\n",
      "    ✓ No fill values detected. Max raw value: 20528\n",
      "2000     244.07               ✓\n",
      "    ✓ No fill values detected. Max raw value: 20362\n",
      "2001     271.05               ✓\n",
      "    ✓ No fill values detected. Max raw value: 19908\n",
      "2002     354.55               ✓\n",
      "    ✓ No fill values detected. Max raw value: 19899\n",
      "2003     369.25               ✓\n",
      "    ✓ No fill values detected. Max raw value: 20608\n",
      "2004     392.21               ✓\n",
      "    ✓ No fill values detected. Max raw value: 20250\n",
      "2005     314.50               ✓\n",
      "    ✓ No fill values detected. Max raw value: 24976\n",
      "2006     345.45               ✓\n",
      "    ✓ No fill values detected. Max raw value: 25159\n",
      "2007     369.80               ✓\n",
      "    ✓ No fill values detected. Max raw value: 25199\n",
      "2008     327.93               ✓\n",
      "    ✓ No fill values detected. Max raw value: 28893\n",
      "2009     310.82               ✓\n",
      "    ✓ No fill values detected. Max raw value: 29876\n",
      "2010     339.28               ✓\n",
      "    ✓ No fill values detected. Max raw value: 26618\n",
      "2011     230.28               ✓\n",
      "    ✓ No fill values detected. Max raw value: 29069\n",
      "2012     251.17               ✓\n",
      "    ✓ No fill values detected. Max raw value: 28721\n",
      "2013     339.44               ✓\n",
      "    ✓ No fill values detected. Max raw value: 30016\n",
      "2014     343.71               ✓\n",
      "    ✓ No fill values detected. Max raw value: 30529\n",
      "2015     413.86               ✓\n",
      "    ✓ No fill values detected. Max raw value: 29738\n",
      "2016     337.23               ✓\n",
      "    ✓ No fill values detected. Max raw value: 28673\n",
      "2017     345.51               ✓\n",
      "    ✓ No fill values detected. Max raw value: 28827\n",
      "2018     394.59               ✓\n",
      "    ✓ No fill values detected. Max raw value: 26888\n",
      "2019     329.34               ✓\n",
      "    ✓ No fill values detected. Max raw value: 28409\n",
      "2020     362.34               ✓\n",
      "    ✓ No fill values detected. Max raw value: 29660\n",
      "2021     365.51               ✓\n",
      "    ✓ No fill values detected. Max raw value: 25492\n",
      "2022     320.03               ✓\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Saving ET results\n",
      "======================================================================\n",
      "✓ ET data saved to: /capstone/aridgw/data/et_timeseries_mexico_aguascalientes_test.csv\n",
      "\n",
      " ET Statistics:\n",
      "  Years with data: 23 / 23\n",
      "  Mean ET: 333.56 mm/yr\n",
      "  Min ET: 230.28 mm/yr\n",
      "  Max ET: 413.86 mm/yr\n",
      "  Std Dev: 47.41 mm/yr\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Joining ET data with groundwater data\n",
      "======================================================================\n",
      "✓ Loaded 20 groundwater records for MexicoAguascaliente\n",
      "\n",
      " Filtering to years 2000-2022...\n",
      "✓ Merged dataset has 2 rows\n",
      "  Rows with ET data: 2\n",
      "  Rows without ET data: 0\n",
      "✓ Merged data saved to: /capstone/aridgw/data/et_gw_merged_mexico_aguascalientes_test.csv\n",
      "\n",
      " Column verification (duplicate columns help verify alignment):\n",
      "     Title Location            location  YEAR   year      Lat  latitude     Lon  longitude tile_id   tile\n",
      "MexicoAguascaliente MexicoAguascaliente  2000 2000.0 22.19816  22.19816 -102.31    -102.31  h08v06 h08v06\n",
      "MexicoAguascaliente MexicoAguascaliente  2001 2001.0 22.19816  22.19816 -102.31    -102.31  h08v06 h08v06\n",
      "\n",
      "======================================================================\n",
      "ET OUTPUT PREVIEW\n",
      "======================================================================\n",
      " year            location  latitude  longitude   tile  mean_ET_mm_yr  buffer_radius_m\n",
      " 2000 MexicoAguascaliente  22.19816    -102.31 h08v06     244.070847             1000\n",
      " 2001 MexicoAguascaliente  22.19816    -102.31 h08v06     271.045837             1000\n",
      " 2002 MexicoAguascaliente  22.19816    -102.31 h08v06     354.550018             1000\n",
      " 2003 MexicoAguascaliente  22.19816    -102.31 h08v06     369.245850             1000\n",
      " 2004 MexicoAguascaliente  22.19816    -102.31 h08v06     392.208344             1000\n",
      " 2005 MexicoAguascaliente  22.19816    -102.31 h08v06     314.500000             1000\n",
      " 2006 MexicoAguascaliente  22.19816    -102.31 h08v06     345.454193             1000\n",
      " 2007 MexicoAguascaliente  22.19816    -102.31 h08v06     369.795807             1000\n",
      " 2008 MexicoAguascaliente  22.19816    -102.31 h08v06     327.929169             1000\n",
      " 2009 MexicoAguascaliente  22.19816    -102.31 h08v06     310.820831             1000\n",
      "\n",
      "======================================================================\n",
      "MERGED OUTPUT PREVIEW\n",
      "======================================================================\n",
      " MEDS_ID  MEDS_ANNUAL_MEDIAN_ROW  YEAR  WaterLevel_m  WaterLevelElev_masl      Title Location      Lat     Lon tile_id   year            location  latitude  longitude   tile  mean_ET_mm_yr  buffer_radius_m\n",
      "      15                     507  2000      85.22369                  NaN MexicoAguascaliente 22.19816 -102.31  h08v06 2000.0 MexicoAguascaliente  22.19816    -102.31 h08v06     244.070847           1000.0\n",
      "      15                     508  2001      89.14079                  NaN MexicoAguascaliente 22.19816 -102.31  h08v06 2001.0 MexicoAguascaliente  22.19816    -102.31 h08v06     271.045837           1000.0\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE!\n",
      "======================================================================\n",
      "ET timeseries: /capstone/aridgw/data/et_timeseries_mexico_aguascalientes_test.csv\n",
      "Merged data: /capstone/aridgw/data/et_gw_merged_mexico_aguascalientes_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Use CSV file with corresponding tile IDs for each site to extract ET timeseries\n",
    "\n",
    "def extract_et_timeseries_single_site():\n",
    "    \"\"\"Extract annual ET timeseries for a single site using existing CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Extracting ET timeseries for: {SITE_NAME}\")\n",
    "    print(f\"Years: {START_YEAR}-{END_YEAR}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Load site info from existing CSV\n",
    "\n",
    "    print(\"\\n STEP 1: Loading site information from CSV\")\n",
    "    \n",
    "    df_sites = pd.read_csv(sites_csv)\n",
    "    \n",
    "    # Filter to just this site\n",
    "    site_data = df_sites[df_sites['Title Location'] == SITE_NAME].iloc[0]\n",
    "    \n",
    "    site_lat = site_data['Lat']\n",
    "    site_lon = site_data['Lon']\n",
    "    tile_id = site_data['tile_id']\n",
    "    \n",
    "    print(f\"✓ Location: {SITE_NAME}\")\n",
    "    print(f\"✓ Coordinates: ({site_lat:.6f}, {site_lon:.6f})\")\n",
    "    print(f\"✓ MODIS Tile: {tile_id}\")\n",
    "    \n",
    "    # Extract h and v from tile_id (e.g., \"h08v05\" -> h=8, v=5)\n",
    "    h = int(tile_id[1:3])\n",
    "    v = int(tile_id[4:6])\n",
    "    \n",
    "    # STEP 2: Create buffer (One at time per site)\n",
    "\n",
    "    print(f\"\\n STEP 2: Creating {BUFFER_RADIUS}m buffer\")\n",
    "    \n",
    "    # modis_crs = CRS.from_proj4(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\") # MODIS Sinusodal OLD CRS\n",
    "    modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs\")\n",
    "    circle_geom = create_circle_buffer(site_lat, site_lon, BUFFER_RADIUS, modis_crs)\n",
    "    print(f\"✓ Buffer created\")\n",
    "    \n",
    "    # STEP 3: Find and filter MODIS files according to tile and year range\n",
    "\n",
    "    print(f\"\\n STEP 3: Finding MODIS files for tile {tile_id}\")\n",
    "    \n",
    "    hdf_files = glob.glob(os.path.join(modis_data_dir, \"*.hdf\")) # Creates variable containing all .hdf files in modis_data_dir\n",
    "    print(f\"✓ Found {len(hdf_files)} total MODIS files\")\n",
    "    \n",
    "    # Parse and filter files for this tile and year range\n",
    "    file_dict = {}  # year -> filepath\n",
    "    for hdf_file in hdf_files: # Looks through all .hdf files in modis_data_dir\n",
    "        info = parse_modis_filename(os.path.basename(hdf_file)) # Parses filename to extract time and tile info\n",
    "        if info and info['tile'] == tile_id and START_YEAR <= info['year'] <= END_YEAR: # Filters files for specified tile and year range\n",
    "            file_dict[info['year']] = {'filepath': hdf_file, 'h': info['h'], 'v': info['v']} # Stores filtered files in dictionary with year as key\n",
    "    \n",
    "    print(f\"✓ Filtered to {len(file_dict)} files for {tile_id} ({START_YEAR}-{END_YEAR})\")\n",
    "    \n",
    "    if len(file_dict) == 0: # If no files found for specified tile and year range\n",
    "        print(f\"\\n WARNING: No files found for tile {tile_id} in year range\")\n",
    "        print(\"Available tiles in directory:\")\n",
    "        for hdf_file in hdf_files[:5]:  # Show first 5 as examples\n",
    "            info = parse_modis_filename(os.path.basename(hdf_file))\n",
    "            if info: \n",
    "                print(f\"  - {info['tile']} (year {info['year']})\")\n",
    "        return None, None\n",
    "    \n",
    "    # STEP 4: Extract ET for each year\n",
    "\n",
    "    print(f\"\\n STEP 4: Extracting ET values\") \n",
    "    print(f\"\\n{'Year':<8} {'Mean ET (mm/yr)':<20} {'Status'}\")\n",
    "    print(\"-\" * 50) \n",
    "    \n",
    "    results = []\n",
    "    for year in range(START_YEAR, END_YEAR + 1): # Loops through each year in specified range\n",
    "        if year in file_dict: \n",
    "            file_info = file_dict[year]\n",
    "            mean_et = extract_et_from_buffer(\n",
    "                file_info['filepath'],\n",
    "                circle_geom,\n",
    "                file_info['h'],\n",
    "                file_info['v']\n",
    "            )\n",
    "            \n",
    "            status = \"✓\" if not np.isnan(mean_et) else \"NO DATA\"\n",
    "            et_display = f\"{mean_et:.2f}\" if not np.isnan(mean_et) else \"NaN\"\n",
    "            print(f\"{year:<8} {et_display:<20} {status}\")\n",
    "            \n",
    "            results.append({ # Appends results to list\n",
    "                'year': year,\n",
    "                'location': SITE_NAME,\n",
    "                'latitude': site_lat,\n",
    "                'longitude': site_lon,\n",
    "                'tile': tile_id,\n",
    "                'mean_ET_mm_yr': mean_et,\n",
    "                'buffer_radius_m': BUFFER_RADIUS\n",
    "            })\n",
    "        else:\n",
    "            print(f\"{year:<8} {'---':<20} FILE MISSING\")\n",
    "            results.append({\n",
    "                'year': year,\n",
    "                'location': SITE_NAME,\n",
    "                'latitude': site_lat,\n",
    "                'longitude': site_lon,\n",
    "                'tile': tile_id,\n",
    "                'mean_ET_mm_yr': np.nan,\n",
    "                'buffer_radius_m': BUFFER_RADIUS\n",
    "            })\n",
    "    \n",
    "    # STEP 5: Save ET results\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Saving ET results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    df_et = pd.DataFrame(results)\n",
    "    df_et = df_et.sort_values('year')\n",
    "    df_et.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"✓ ET data saved to: {output_csv}\")\n",
    "    print(f\"\\n ET Statistics:\")\n",
    "    print(f\"  Years with data: {df_et['mean_ET_mm_yr'].notna().sum()} / {len(df_et)}\")\n",
    "    \n",
    "    if df_et['mean_ET_mm_yr'].notna().sum() > 0:\n",
    "        print(f\"  Mean ET: {df_et['mean_ET_mm_yr'].mean():.2f} mm/yr\")\n",
    "        print(f\"  Min ET: {df_et['mean_ET_mm_yr'].min():.2f} mm/yr\")\n",
    "        print(f\"  Max ET: {df_et['mean_ET_mm_yr'].max():.2f} mm/yr\")\n",
    "        print(f\"  Std Dev: {df_et['mean_ET_mm_yr'].std():.2f} mm/yr\")\n",
    "    \n",
    "    # STEP 6: JOIN with groundwater data\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: Joining ET data with groundwater data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the full groundwater dataset\n",
    "    df_gw = pd.read_csv(sites_csv)\n",
    "    \n",
    "    # Filter to only this site\n",
    "    df_gw_site = df_gw[df_gw['Title Location'] == SITE_NAME].copy()\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df_gw_site)} groundwater records for {SITE_NAME}\")\n",
    "    \n",
    "    # Perform the join\n",
    "    # Join on: location (from ET) = Title Location (from GW) AND year (ET) = YEAR (GW)\n",
    "    df_merged_all_years = pd.merge(\n",
    "        df_gw_site,\n",
    "        df_et,\n",
    "        left_on=['Title Location', 'YEAR'],\n",
    "        right_on=['location', 'year'],\n",
    "        how='left'  # Keep all groundwater records, add ET where available\n",
    "    )\n",
    "    # Filter to specified year range\n",
    "    print(f\"\\n Filtering to years {START_YEAR}-{END_YEAR}...\")\n",
    "\n",
    "    df_merged = df_merged_all_years[ # Filters merged dataframe to specified year range\n",
    "    (df_merged_all_years['YEAR'] >= START_YEAR) & \n",
    "    (df_merged_all_years['YEAR'] <= END_YEAR)\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"✓ Merged dataset has {len(df_merged)} rows\")\n",
    "    print(f\"  Rows with ET data: {df_merged['mean_ET_mm_yr'].notna().sum()}\")\n",
    "    print(f\"  Rows without ET data: {df_merged['mean_ET_mm_yr'].isna().sum()}\")\n",
    "    \n",
    "    # Save merged dataset\n",
    "    df_merged.to_csv(merged_output_csv, index=False)\n",
    "    print(f\"✓ Merged data saved to: {merged_output_csv}\")\n",
    "    \n",
    "    # Show column comparison for verification\n",
    "    print(\"\\n Column verification (duplicate columns help verify alignment):\")\n",
    "    verification_cols = ['Title Location', 'location', 'YEAR', 'year', \n",
    "                        'Lat', 'latitude', 'Lon', 'longitude', \n",
    "                        'tile_id', 'tile']\n",
    "    available_cols = [col for col in verification_cols if col in df_merged.columns]\n",
    "    if available_cols:\n",
    "        print(df_merged[available_cols].head(5).to_string(index=False))\n",
    "    \n",
    "    return df_et, df_merged\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\":\n",
    "    df_et, df_merged = extract_et_timeseries_single_site()\n",
    "    \n",
    "    if df_et is not None and df_merged is not None:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ET OUTPUT PREVIEW\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_et.head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MERGED OUTPUT PREVIEW\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_merged.head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PROCESSING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"ET timeseries: {output_csv}\")\n",
    "        print(f\"Merged data: {merged_output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AridGW-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
