{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d425741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyhdf.SD import SD, SDC\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "from pyproj import CRS\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4246e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_site_name(site_name):\n",
    "    \"\"\"Convert site name to lowercase, replace spaces with _, remove special chars\"\"\"\n",
    "    # Convert to lowercase\n",
    "    cleaned = site_name.lower()\n",
    "    # Replace spaces with underscores\n",
    "    cleaned = cleaned.replace(' ', '_')\n",
    "    # Remove everything that's not a letter, number, or underscore\n",
    "    cleaned = re.sub(r'[^a-z0-9_]', '', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d7afe5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT INPUTS BASED ON SITE OF INTEREST AND YEAR RANGE\n",
    "\n",
    "# Single site to process\n",
    "SITE_NAME = \"Southern Willcox Basin\"\n",
    "\n",
    "# Year range\n",
    "START_YEAR = 2000\n",
    "END_YEAR = 2025\n",
    "\n",
    "# Create clean filename version\n",
    "clean_name = clean_site_name(SITE_NAME)\n",
    "\n",
    "# Paths\n",
    "sites_csv = \"/capstone/aridgw/data/site_data/2026_02_06_annual_median_groundwater_levels_5newAZsites_tile_id.csv\"\n",
    "modis_data_dir = \"/capstone/aridgw/data/modis_data/\"\n",
    "output_csv = f\"/capstone/aridgw/data/et_timeseries_{clean_name}_2km.csv\"\n",
    "merged_output_csv = f\"/capstone/aridgw/data/et_gw_merged_{clean_name}_2km.csv\"\n",
    "\n",
    "# Buffer radius in meters\n",
    "BUFFER_RADIUS = 2000  # 2 km\n",
    "\n",
    "# MODIS constants\n",
    "TILE_SIZE = 1111950.0\n",
    "X_MIN = -20015109.354\n",
    "Y_MAX = 10007554.677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ee6cca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real MODIS filename example \n",
    "# MOD16A3GF.A2000001.h00v08.061.2020264071747.hdf\n",
    "# MOD16A3GF MODIS data naming convention \n",
    "# 2000 Year \n",
    "# Day of year 001\n",
    "# h00v08 Tile h=0, v=8\n",
    "# 061 = Collection number\n",
    "# 2020264071747 = Production date\n",
    "# {} Ensures there is a match for the specified number of digits\n",
    "# ({}) Captures name as a group after ensuring there is a match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "bb162a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used in processing\n",
    "\n",
    "def parse_modis_filename(filename):\n",
    "    \"\"\"Extract year, tile, h, v from MODIS filename\"\"\"\n",
    "    # Naming convention first \n",
    "    pattern = r'MOD16A3GF\\.A(\\d{4})\\d{3}\\.h(\\d{2})v(\\d{2})\\.\\d{3}\\.\\d+\\.hdf' # Defines regex pattern for MODIS filenames containing time and tile info\n",
    "    match = re.search(pattern, filename) # Looks through modis_data_dir for files that match the pattern\n",
    "    \n",
    "    if match:\n",
    "        return { # Returns a dictionary with extracted time and tile info\n",
    "            'year': int(match.group(1)),\n",
    "            'tile': f\"h{match.group(2)}v{match.group(3)}\",\n",
    "            'h': int(match.group(2)),\n",
    "            'v': int(match.group(3))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# def create_circle_buffer(center_lat, center_lon, radius_meters, target_crs):\n",
    "  #   \"\"\"Create circular buffer around site point\n",
    "    #    - Input lat/lon in degrees\n",
    "     #   - radius in meters\n",
    "      #  - target_crs: pyproj CRS object for target projection (MODIS Sinusoidal)\n",
    "    # \"\"\"\n",
    "    # point = gpd.GeoSeries([Point(center_lon, center_lat)], crs=\"EPSG:4326\") # CRS is EPSG:4326 by default since lat/lon are used\n",
    "    # point_projected = point.to_crs(epsg=3857) # Project to Web Mercator for accurate buffering in meters\n",
    "    # circle = point_projected.buffer(radius_meters) # Create buffer in meters\n",
    "    #circle_target = circle.to_crs(target_crs) # Convert to target CRS (MODIS Sinusoidal)\n",
    "    # return circle_target\n",
    "\n",
    "def create_circle_buffer(center_lat, center_lon, radius_meters, target_crs):\n",
    "    \"\"\"Create circular buffer around site point\"\"\"\n",
    "    point = gpd.GeoSeries([Point(center_lon, center_lat)], crs=\"EPSG:4326\")\n",
    "    # Project directly to target CRS (MODIS) \n",
    "    point_projected = point.to_crs(target_crs)\n",
    "    circle = point_projected.buffer(radius_meters)\n",
    "    return circle\n",
    "\n",
    "def extract_et_from_buffer(hdf_file, circle_geom, h, v):\n",
    "    \"\"\"Extract mean ET from buffer\n",
    "    - Inputs:\n",
    "        - hdf_file: path to MODIS HDF file\n",
    "        - circle_geom: output geometry of create_circle_buffer function in MODIS CRS\n",
    "        - h, v: tile indices\n",
    "    - Returns:\n",
    "        - mean ET value within buffer (float, mm/yr), or np.nan if no data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open HDF\n",
    "        hdf = SD(hdf_file, SDC.READ)\n",
    "        et_dataset = hdf.select('ET_500m')\n",
    "        scale_factor = et_dataset.scale_factor\n",
    "        fill_value = et_dataset.attributes()['_FillValue'] # Fill values should have been identified, they will turn into NaN\n",
    "        \n",
    "        # Read and clean data\n",
    "        et_data_raw = et_dataset[:].astype(float) # Turn into float for NaN handling\n",
    "        et_data_raw[et_data_raw == fill_value] = np.nan # Set fill values to NaN\n",
    "        et_data_raw[et_data_raw >= 65500] = np.nan # Additional cleaning for unrealistic values\n",
    "        et_data_raw[et_data_raw < 0] = np.nan # Additional cleaning for negative values\n",
    "        remaining_suspicious = et_data_raw[(et_data_raw >= 60000) & (~np.isnan(et_data_raw))]\n",
    "        if len(remaining_suspicious) > 0:\n",
    "            print(f\"  WARNING: {len(remaining_suspicious)} pixels with values >= 60,000 remain after filtering!\")\n",
    "            print(f\"  Values: {np.unique(remaining_suspicious)}\")\n",
    "        else:\n",
    "            valid_count = np.sum(~np.isnan(et_data_raw))\n",
    "            if valid_count > 0:\n",
    "                print(f\"    ✓ No fill values detected. Max raw value: {np.nanmax(et_data_raw):.0f}\")\n",
    "        et_data = et_data_raw * scale_factor # Scale to (mm/yr) units using scaling factor\n",
    "        \n",
    "        # Setup geotransform\n",
    "        PIXELS = et_data.shape[0] # Number of pixels in one dimension (assuming square)\n",
    "        RES = TILE_SIZE / PIXELS # Calculate resolution, how many meters per pixel\n",
    "        x_ul = X_MIN + h * TILE_SIZE # Upper-left x coordinate\n",
    "        y_ul = Y_MAX - v * TILE_SIZE # Upper-left y coordinate\n",
    "        transform = from_origin(x_ul, y_ul, RES, RES) # Create transformation\n",
    "        # modis_crs = CRS.from_proj4(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\") # MODIS Sinusoidal CRS OLD\n",
    "        # modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs +type=crs\")\n",
    "        modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs\") # MODIS Sinusoidal CRS\n",
    "        \n",
    "        # Create profile and clip\n",
    "        profile = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": et_data.shape[0], # Pixels in y direction\n",
    "            \"width\": et_data.shape[1], # Pixels in x direction\n",
    "            \"count\": 1, # of bands\n",
    "            \"dtype\": \"float32\", # Data type\n",
    "            \"crs\": modis_crs, # CRS info\n",
    "            \"transform\": transform, # Transformation info\n",
    "            \"nodata\": np.nan\n",
    "        }\n",
    "        \n",
    "        with MemoryFile() as memfile: # Use in-memory file for rasterio operations for efficiency\n",
    "            with memfile.open(**profile) as dataset:\n",
    "                dataset.write(et_data.astype(\"float32\"), 1)\n",
    "                clipped, _ = mask(dataset, circle_geom.geometry, crop=True, all_touched=True) # Clip to buffer area, have all ET pixels touched by buffer included\n",
    "        \n",
    "        # Calculate mean\n",
    "        clipped_valid = clipped[0][~np.isnan(clipped[0])] # Valid (non-NaN) values only\n",
    "        # Conduct mean zonal statistics\n",
    "        mean_et = clipped_valid.mean() if len(clipped_valid) > 0 else np.nan # Mean ET or NaN if no valid data\n",
    "        \n",
    "        hdf.end()\n",
    "        return mean_et\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e8bb86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Extracting ET timeseries for: Southern Willcox Basin\n",
      "Years: 2000-2025\n",
      "======================================================================\n",
      "\n",
      " STEP 1: Loading site information from CSV\n",
      "✓ Location: Southern Willcox Basin\n",
      "✓ Coordinates: (31.365972, -109.662944)\n",
      "✓ MODIS Tile: h08v05\n",
      "\n",
      " STEP 2: Creating 2000m buffer\n",
      "✓ Buffer created\n",
      "\n",
      " STEP 3: Finding MODIS files for tile h08v05\n",
      "✓ Found 7139 total MODIS files\n",
      "✓ Filtered to 25 files for h08v05 (2000-2025)\n",
      "\n",
      " STEP 4: Extracting ET values\n",
      "\n",
      "Year     Mean ET (mm/yr)      Status\n",
      "--------------------------------------------------\n",
      "    ✓ No fill values detected. Max raw value: 15069\n",
      "2000     104.51               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14740\n",
      "2001     121.89               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14316\n",
      "2002     102.25               ✓\n",
      "    ✓ No fill values detected. Max raw value: 15130\n",
      "2003     91.87                ✓\n",
      "    ✓ No fill values detected. Max raw value: 14627\n",
      "2004     119.28               ✓\n",
      "    ✓ No fill values detected. Max raw value: 15060\n",
      "2005     124.70               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18420\n",
      "2006     124.38               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14889\n",
      "2007     128.84               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14244\n",
      "2008     138.33               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14209\n",
      "2009     84.53                ✓\n",
      "    ✓ No fill values detected. Max raw value: 13953\n",
      "2010     143.49               ✓\n",
      "    ✓ No fill values detected. Max raw value: 19353\n",
      "2011     103.96               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14815\n",
      "2012     100.33               ✓\n",
      "    ✓ No fill values detected. Max raw value: 15040\n",
      "2013     159.77               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14906\n",
      "2014     164.05               ✓\n",
      "    ✓ No fill values detected. Max raw value: 19071\n",
      "2015     199.69               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14028\n",
      "2016     159.60               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18180\n",
      "2017     182.64               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18196\n",
      "2018     165.11               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18317\n",
      "2019     220.73               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18685\n",
      "2020     165.16               ✓\n",
      "    ✓ No fill values detected. Max raw value: 18501\n",
      "2021     202.34               ✓\n",
      "    ✓ No fill values detected. Max raw value: 14091\n",
      "2022     234.10               ✓\n",
      "    ✓ No fill values detected. Max raw value: 23396\n",
      "2023     145.60               ✓\n",
      "    ✓ No fill values detected. Max raw value: 19718\n",
      "2024     148.10               ✓\n",
      "2025     ---                  FILE MISSING\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Saving ET results\n",
      "======================================================================\n",
      "✓ ET data saved to: /capstone/aridgw/data/et_timeseries_southern_willcox_basin_2km.csv\n",
      "\n",
      " ET Statistics:\n",
      "  Years with data: 25 / 26\n",
      "  Mean ET: 145.41 mm/yr\n",
      "  Min ET: 84.53 mm/yr\n",
      "  Max ET: 234.10 mm/yr\n",
      "  Std Dev: 40.28 mm/yr\n",
      "\n",
      "======================================================================\n",
      "STEP 6: Joining ET data with groundwater data\n",
      "======================================================================\n",
      "✓ Loaded 19 groundwater records for Southern Willcox Basin\n",
      "\n",
      " Filtering to years 2000-2025...\n",
      "✓ Merged dataset has 10 rows\n",
      "  Rows with ET data: 10\n",
      "  Rows without ET data: 0\n",
      "✓ Merged data saved to: /capstone/aridgw/data/et_gw_merged_southern_willcox_basin_2km.csv\n",
      "\n",
      " Column verification (duplicate columns help verify alignment):\n",
      "        Title Location               location  YEAR   year       Lat  latitude         Lon   longitude tile_id   tile\n",
      "Southern Willcox Basin Southern Willcox Basin  2006 2006.0 31.365972 31.365972 -109.662944 -109.662944  h08v05 h08v05\n",
      "Southern Willcox Basin Southern Willcox Basin  2011 2011.0 31.365972 31.365972 -109.662944 -109.662944  h08v05 h08v05\n",
      "Southern Willcox Basin Southern Willcox Basin  2012 2012.0 31.365972 31.365972 -109.662944 -109.662944  h08v05 h08v05\n",
      "Southern Willcox Basin Southern Willcox Basin  2013 2013.0 31.365972 31.365972 -109.662944 -109.662944  h08v05 h08v05\n",
      "Southern Willcox Basin Southern Willcox Basin  2015 2015.0 31.365972 31.365972 -109.662944 -109.662944  h08v05 h08v05\n",
      "\n",
      "======================================================================\n",
      "ET OUTPUT PREVIEW\n",
      "======================================================================\n",
      " year               location  latitude   longitude   tile  mean_ET_mm_yr  buffer_radius_m\n",
      " 2000 Southern Willcox Basin 31.365972 -109.662944 h08v05     104.507591             2000\n",
      " 2001 Southern Willcox Basin 31.365972 -109.662944 h08v05     121.893669             2000\n",
      " 2002 Southern Willcox Basin 31.365972 -109.662944 h08v05     102.248093             2000\n",
      " 2003 Southern Willcox Basin 31.365972 -109.662944 h08v05      91.872154             2000\n",
      " 2004 Southern Willcox Basin 31.365972 -109.662944 h08v05     119.283546             2000\n",
      " 2005 Southern Willcox Basin 31.365972 -109.662944 h08v05     124.696190             2000\n",
      " 2006 Southern Willcox Basin 31.365972 -109.662944 h08v05     124.383545             2000\n",
      " 2007 Southern Willcox Basin 31.365972 -109.662944 h08v05     128.839218             2000\n",
      " 2008 Southern Willcox Basin 31.365972 -109.662944 h08v05     138.330368             2000\n",
      " 2009 Southern Willcox Basin 31.365972 -109.662944 h08v05      84.531647             2000\n",
      "\n",
      "======================================================================\n",
      "MERGED OUTPUT PREVIEW\n",
      "======================================================================\n",
      " MEDS_ID  MEDS_ANNUAL_MEDIAN_ROW  YEAR  WaterLevel_m  WaterLevelElev_masl         Title Location       Lat         Lon tile_id   year               location  latitude   longitude   tile  mean_ET_mm_yr  buffer_radius_m\n",
      "      21                     693  2006      42.88536                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2006.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     124.383545           2000.0\n",
      "      21                     694  2011      40.44696                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2011.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     103.956955           2000.0\n",
      "      21                     695  2012      40.23360                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2012.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     100.326576           2000.0\n",
      "      21                     696  2013      40.08120                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2013.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     159.768356           2000.0\n",
      "      21                     697  2015      40.38600                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2015.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     199.688614           2000.0\n",
      "      21                     698  2016      41.05656                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2016.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     159.598724           2000.0\n",
      "      21                     699  2018      44.80560                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2018.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     165.112686           2000.0\n",
      "      21                     700  2019      46.63440                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2019.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     220.726562           2000.0\n",
      "      21                     701  2020      47.15256                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2020.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     165.159470           2000.0\n",
      "      21                     702  2022      47.33544                  NaN Southern Willcox Basin 31.365972 -109.662944  h08v05 2022.0 Southern Willcox Basin 31.365972 -109.662944 h08v05     234.096191           2000.0\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE!\n",
      "======================================================================\n",
      "ET timeseries: /capstone/aridgw/data/et_timeseries_southern_willcox_basin_2km.csv\n",
      "Merged data: /capstone/aridgw/data/et_gw_merged_southern_willcox_basin_2km.csv\n"
     ]
    }
   ],
   "source": [
    "# Use CSV file with corresponding tile IDs for each site to extract ET timeseries\n",
    "\n",
    "def extract_et_timeseries_single_site():\n",
    "    \"\"\"Extract annual ET timeseries for a single site using existing CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Extracting ET timeseries for: {SITE_NAME}\")\n",
    "    print(f\"Years: {START_YEAR}-{END_YEAR}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Load site info from existing CSV\n",
    "\n",
    "    print(\"\\n STEP 1: Loading site information from CSV\")\n",
    "    \n",
    "    df_sites = pd.read_csv(sites_csv)\n",
    "    \n",
    "    # Filter to just this site\n",
    "    site_data = df_sites[df_sites['Title Location'] == SITE_NAME].iloc[0]\n",
    "    \n",
    "    site_lat = site_data['Lat']\n",
    "    site_lon = site_data['Lon']\n",
    "    tile_id = site_data['tile_id']\n",
    "    \n",
    "    print(f\"✓ Location: {SITE_NAME}\")\n",
    "    print(f\"✓ Coordinates: ({site_lat:.6f}, {site_lon:.6f})\")\n",
    "    print(f\"✓ MODIS Tile: {tile_id}\")\n",
    "    \n",
    "    # Extract h and v from tile_id (e.g., \"h08v05\" -> h=8, v=5)\n",
    "    h = int(tile_id[1:3])\n",
    "    v = int(tile_id[4:6])\n",
    "    \n",
    "    # STEP 2: Create buffer (One at time per site)\n",
    "\n",
    "    print(f\"\\n STEP 2: Creating {BUFFER_RADIUS}m buffer\")\n",
    "    \n",
    "    # modis_crs = CRS.from_proj4(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\") # MODIS Sinusodal OLD CRS\n",
    "    modis_crs = CRS.from_proj4(\"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs\")\n",
    "    circle_geom = create_circle_buffer(site_lat, site_lon, BUFFER_RADIUS, modis_crs)\n",
    "    print(f\"✓ Buffer created\")\n",
    "    \n",
    "    # STEP 3: Find and filter MODIS files according to tile and year range\n",
    "\n",
    "    print(f\"\\n STEP 3: Finding MODIS files for tile {tile_id}\")\n",
    "    \n",
    "    hdf_files = glob.glob(os.path.join(modis_data_dir, \"*.hdf\")) # Creates variable containing all .hdf files in modis_data_dir\n",
    "    print(f\"✓ Found {len(hdf_files)} total MODIS files\")\n",
    "    \n",
    "    # Parse and filter files for this tile and year range\n",
    "    file_dict = {}  # year -> filepath\n",
    "    for hdf_file in hdf_files: # Looks through all .hdf files in modis_data_dir\n",
    "        info = parse_modis_filename(os.path.basename(hdf_file)) # Parses filename to extract time and tile info\n",
    "        if info and info['tile'] == tile_id and START_YEAR <= info['year'] <= END_YEAR: # Filters files for specified tile and year range\n",
    "            file_dict[info['year']] = {'filepath': hdf_file, 'h': info['h'], 'v': info['v']} # Stores filtered files in dictionary with year as key\n",
    "    \n",
    "    print(f\"✓ Filtered to {len(file_dict)} files for {tile_id} ({START_YEAR}-{END_YEAR})\")\n",
    "    \n",
    "    if len(file_dict) == 0: # If no files found for specified tile and year range\n",
    "        print(f\"\\n WARNING: No files found for tile {tile_id} in year range\")\n",
    "        print(\"Available tiles in directory:\")\n",
    "        for hdf_file in hdf_files[:5]:  # Show first 5 as examples\n",
    "            info = parse_modis_filename(os.path.basename(hdf_file))\n",
    "            if info: \n",
    "                print(f\"  - {info['tile']} (year {info['year']})\")\n",
    "        return None, None\n",
    "    \n",
    "    # STEP 4: Extract ET for each year\n",
    "\n",
    "    print(f\"\\n STEP 4: Extracting ET values\") \n",
    "    print(f\"\\n{'Year':<8} {'Mean ET (mm/yr)':<20} {'Status'}\")\n",
    "    print(\"-\" * 50) \n",
    "    \n",
    "    results = []\n",
    "    for year in range(START_YEAR, END_YEAR + 1): # Loops through each year in specified range\n",
    "        if year in file_dict: \n",
    "            file_info = file_dict[year]\n",
    "            mean_et = extract_et_from_buffer(\n",
    "                file_info['filepath'],\n",
    "                circle_geom,\n",
    "                file_info['h'],\n",
    "                file_info['v']\n",
    "            )\n",
    "            \n",
    "            status = \"✓\" if not np.isnan(mean_et) else \"NO DATA\"\n",
    "            et_display = f\"{mean_et:.2f}\" if not np.isnan(mean_et) else \"NaN\"\n",
    "            print(f\"{year:<8} {et_display:<20} {status}\")\n",
    "            \n",
    "            results.append({ # Appends results to list\n",
    "                'year': year,\n",
    "                'location': SITE_NAME,\n",
    "                'latitude': site_lat,\n",
    "                'longitude': site_lon,\n",
    "                'tile': tile_id,\n",
    "                'mean_ET_mm_yr': mean_et,\n",
    "                'buffer_radius_m': BUFFER_RADIUS\n",
    "            })\n",
    "        else:\n",
    "            print(f\"{year:<8} {'---':<20} FILE MISSING\")\n",
    "            results.append({\n",
    "                'year': year,\n",
    "                'location': SITE_NAME,\n",
    "                'latitude': site_lat,\n",
    "                'longitude': site_lon,\n",
    "                'tile': tile_id,\n",
    "                'mean_ET_mm_yr': np.nan,\n",
    "                'buffer_radius_m': BUFFER_RADIUS\n",
    "            })\n",
    "    \n",
    "    # STEP 5: Save ET results\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Saving ET results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    df_et = pd.DataFrame(results)\n",
    "    df_et = df_et.sort_values('year')\n",
    "    df_et.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"✓ ET data saved to: {output_csv}\")\n",
    "    print(f\"\\n ET Statistics:\")\n",
    "    print(f\"  Years with data: {df_et['mean_ET_mm_yr'].notna().sum()} / {len(df_et)}\")\n",
    "    \n",
    "    if df_et['mean_ET_mm_yr'].notna().sum() > 0:\n",
    "        print(f\"  Mean ET: {df_et['mean_ET_mm_yr'].mean():.2f} mm/yr\")\n",
    "        print(f\"  Min ET: {df_et['mean_ET_mm_yr'].min():.2f} mm/yr\")\n",
    "        print(f\"  Max ET: {df_et['mean_ET_mm_yr'].max():.2f} mm/yr\")\n",
    "        print(f\"  Std Dev: {df_et['mean_ET_mm_yr'].std():.2f} mm/yr\")\n",
    "    \n",
    "    # STEP 6: JOIN with groundwater data\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: Joining ET data with groundwater data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the full groundwater dataset\n",
    "    df_gw = pd.read_csv(sites_csv)\n",
    "    \n",
    "    # Filter to only this site\n",
    "    df_gw_site = df_gw[df_gw['Title Location'] == SITE_NAME].copy()\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df_gw_site)} groundwater records for {SITE_NAME}\")\n",
    "    \n",
    "    # Perform the join\n",
    "    # Join on: location (from ET) = Title Location (from GW) AND year (ET) = YEAR (GW)\n",
    "    df_merged_all_years = pd.merge(\n",
    "        df_gw_site,\n",
    "        df_et,\n",
    "        left_on=['Title Location', 'YEAR'],\n",
    "        right_on=['location', 'year'],\n",
    "        how='left'  # Keep all groundwater records, add ET where available\n",
    "    )\n",
    "    # Filter to specified year range\n",
    "    print(f\"\\n Filtering to years {START_YEAR}-{END_YEAR}...\")\n",
    "\n",
    "    df_merged = df_merged_all_years[ # Filters merged dataframe to specified year range\n",
    "    (df_merged_all_years['YEAR'] >= START_YEAR) & \n",
    "    (df_merged_all_years['YEAR'] <= END_YEAR)\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"✓ Merged dataset has {len(df_merged)} rows\")\n",
    "    print(f\"  Rows with ET data: {df_merged['mean_ET_mm_yr'].notna().sum()}\")\n",
    "    print(f\"  Rows without ET data: {df_merged['mean_ET_mm_yr'].isna().sum()}\")\n",
    "    \n",
    "    # Save merged dataset\n",
    "    df_merged.to_csv(merged_output_csv, index=False)\n",
    "    print(f\"✓ Merged data saved to: {merged_output_csv}\")\n",
    "    \n",
    "    # Show column comparison for verification\n",
    "    print(\"\\n Column verification (duplicate columns help verify alignment):\")\n",
    "    verification_cols = ['Title Location', 'location', 'YEAR', 'year', \n",
    "                        'Lat', 'latitude', 'Lon', 'longitude', \n",
    "                        'tile_id', 'tile']\n",
    "    available_cols = [col for col in verification_cols if col in df_merged.columns]\n",
    "    if available_cols:\n",
    "        print(df_merged[available_cols].head(5).to_string(index=False))\n",
    "    \n",
    "    return df_et, df_merged\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\":\n",
    "    df_et, df_merged = extract_et_timeseries_single_site()\n",
    "    \n",
    "    if df_et is not None and df_merged is not None:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ET OUTPUT PREVIEW\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_et.head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MERGED OUTPUT PREVIEW\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_merged.head(10).to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PROCESSING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"ET timeseries: {output_csv}\")\n",
    "        print(f\"Merged data: {merged_output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AridGW-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
